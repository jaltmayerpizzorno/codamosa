# Automatically generated by Pynguin.
import blib2to3.pgen2.tokenize as module_0
import blib2to3.pgen2.grammar as module_1

def test_case_0():
    try:
        int_0 = None
        str_0 = '[1-9]\\d*(?:_\\d+)*[lL]?'
        str_1 = 'n)Yq`/'
        var_0 = module_0.maybe()
        dict_0 = {str_0: int_0, str_0: str_0, str_1: str_0, str_1: str_1}
        stop_tokenizing_0 = module_0.StopTokenizing()
        tuple_0 = (int_0, stop_tokenizing_0)
        grammar_0 = module_1.Grammar()
        var_1 = grammar_0.copy()
        var_2 = module_0.printtoken(int_0, dict_0, tuple_0, dict_0, var_1)
    except BaseException:
        pass

def test_case_1():
    try:
        callable_0 = None
        module_0.tokenize(callable_0)
    except BaseException:
        pass

def test_case_2():
    try:
        iterable_0 = None
        str_0 = module_0.untokenize(iterable_0)
    except BaseException:
        pass

def test_case_3():
    try:
        str_0 = 'uAaP"<csBO\\LZIJFu/'
        str_1 = module_0.untokenize(str_0)
    except BaseException:
        pass

def test_case_4():
    try:
        list_0 = []
        var_0 = module_0.any()
        str_0 = module_0.untokenize(list_0)
        var_1 = module_0.any()
        callable_0 = None
        token_error_0 = module_0.TokenError()
        module_0.tokenize(callable_0)
    except BaseException:
        pass

def test_case_5():
    try:
        int_0 = 29
        str_0 = '__main__'
        tuple_0 = (int_0, str_0)
        bytes_0 = None
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.compat(tuple_0, bytes_0)
    except BaseException:
        pass

def test_case_6():
    try:
        int_0 = 1128
        str_0 = '8L^-\tnYpNyf?\t\txGt!6'
        tuple_0 = (int_0, str_0)
        list_0 = None
        tuple_1 = (str_0, list_0)
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.compat(tuple_0, tuple_1)
    except BaseException:
        pass

def test_case_7():
    try:
        set_0 = None
        tuple_0 = module_0.detect_encoding(set_0)
    except BaseException:
        pass

def test_case_8():
    try:
        untokenizer_0 = module_0.Untokenizer()
        int_0 = 1418
        tuple_0 = (int_0, int_0)
        untokenizer_0.add_whitespace(tuple_0)
    except BaseException:
        pass

def test_case_9():
    try:
        list_0 = []
        var_0 = module_0.any()
        str_0 = module_0.untokenize(list_0)
        var_1 = module_0.any()
        int_0 = None
        var_2 = module_0.any(*list_0)
        callable_0 = None
        set_0 = set()
        iterator_0 = module_0.generate_tokens(callable_0)
        tuple_0 = (set_0,)
        iterator_1 = module_0.generate_tokens(callable_0, tuple_0)
        untokenizer_0 = module_0.Untokenizer()
        tuple_1 = (int_0, str_0)
        untokenizer_0.compat(tuple_1, iterator_1)
    except BaseException:
        pass

def test_case_10():
    try:
        list_0 = []
        var_0 = module_0.any()
        str_0 = module_0.untokenize(list_0)
        int_0 = -2
        tuple_0 = (int_0, int_0)
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.add_whitespace(tuple_0)
        tuple_1 = (int_0, str_0)
        bytes_0 = b'\x05\xb8\x97\x1f\xeb\xf5Y\x1a1\xcc\xa3\x85I\xfa*\xff'
        list_1 = [tuple_0, tuple_1, bytes_0]
        untokenizer_1 = module_0.Untokenizer()
        str_1 = untokenizer_1.untokenize(list_1)
    except BaseException:
        pass

def test_case_11():
    try:
        list_0 = []
        var_0 = module_0.any()
        str_0 = module_0.untokenize(list_0)
        int_0 = 2
        tuple_0 = (int_0, int_0)
        untokenizer_0 = module_0.Untokenizer()
        tuple_1 = (int_0, str_0)
        dict_0 = {}
        untokenizer_0.compat(tuple_1, dict_0)
        untokenizer_0.add_whitespace(tuple_0)
    except BaseException:
        pass

def test_case_12():
    try:
        list_0 = []
        var_0 = module_0.any()
        str_0 = module_0.untokenize(list_0)
        stop_tokenizing_0 = module_0.StopTokenizing(*list_0)
        int_0 = 4
        tuple_0 = (int_0, int_0)
        untokenizer_0 = module_0.Untokenizer()
        tuple_1 = (int_0, str_0)
        dict_0 = {}
        untokenizer_0.compat(tuple_1, dict_0)
        untokenizer_0.add_whitespace(tuple_0)
    except BaseException:
        pass

def test_case_13():
    try:
        var_0 = module_0.group()
        int_0 = 4
        str_0 = 'L'
        tuple_0 = (int_0, str_0)
        dict_0 = {tuple_0: tuple_0, var_0: tuple_0}
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.compat(tuple_0, dict_0)
        var_1 = module_0.group()
        str_1 = 'h'
        str_2 = ''
        list_0 = [str_1, str_1]
        str_3 = 'l_\t5k20D.'
        dict_1 = {str_3: list_0, str_2: str_1}
        token_error_0 = module_0.TokenError(*list_0, **dict_1)
    except BaseException:
        pass

def test_case_14():
    try:
        list_0 = []
        var_0 = module_0.any()
        str_0 = module_0.untokenize(list_0)
        stop_tokenizing_0 = module_0.StopTokenizing(*list_0)
        int_0 = 1
        tuple_0 = (int_0, int_0)
        untokenizer_0 = module_0.Untokenizer()
        tuple_1 = (int_0, str_0)
        dict_0 = {}
        untokenizer_0.compat(tuple_1, dict_0)
        untokenizer_0.add_whitespace(tuple_0)
        tuple_2 = (int_0, str_0)
        bytes_0 = b'\x05\xb8\x97\x1f\xeb\xf5Y\x1a1\xcc\xa3\x85I\xfa*\xff'
        tuple_3 = (int_0, str_0)
        list_1 = [tuple_0, tuple_2, bytes_0]
        untokenizer_0.compat(tuple_3, list_1)
    except BaseException:
        pass