# Automatically generated by Pynguin.
import blib2to3.pgen2.tokenize as module_0
import blib2to3.pgen2.grammar as module_1

def test_case_0():
    try:
        callable_0 = None
        module_0.tokenize(callable_0)
    except BaseException:
        pass

def test_case_1():
    try:
        str_0 = '&kp?L=s'
        str_1 = module_0.untokenize(str_0)
    except BaseException:
        pass

def test_case_2():
    try:
        int_0 = 59
        str_0 = '__main__'
        tuple_0 = (int_0, str_0)
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.compat(tuple_0, untokenizer_0)
    except BaseException:
        pass

def test_case_3():
    try:
        int_0 = -649
        str_0 = '>2JwC>?InUoCJ\x0c"'
        tuple_0 = (int_0, str_0)
        list_0 = [str_0, tuple_0, int_0]
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.compat(tuple_0, list_0)
    except BaseException:
        pass

def test_case_4():
    try:
        str_0 = 'M'
        tuple_0 = module_0.detect_encoding(str_0)
    except BaseException:
        pass

def test_case_5():
    try:
        list_0 = []
        dict_0 = {}
        token_error_0 = module_0.TokenError(*list_0, **dict_0)
        token_error_1 = module_0.TokenError()
        int_0 = 1332
        str_0 = 'F\ru2q'
        tuple_0 = (int_0, str_0)
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.compat(tuple_0, dict_0)
        untokenizer_1 = module_0.Untokenizer()
        stop_tokenizing_0 = module_0.StopTokenizing(*list_0)
        tuple_1 = (int_0, int_0)
        untokenizer_1.add_whitespace(tuple_1)
    except BaseException:
        pass

def test_case_6():
    try:
        dict_0 = {}
        str_0 = module_0.untokenize(dict_0)
        callable_0 = None
        module_0.tokenize(callable_0)
    except BaseException:
        pass

def test_case_7():
    try:
        var_0 = module_0.group()
        list_0 = [var_0]
        dict_0 = {}
        token_error_0 = module_0.TokenError(*list_0, **dict_0)
        var_1 = module_0.any()
        token_error_1 = module_0.TokenError()
        untokenizer_0 = module_0.Untokenizer()
        var_2 = module_0.any()
        callable_0 = None
        int_0 = -2428
        tuple_0 = (int_0, int_0)
        untokenizer_0.add_whitespace(tuple_0)
        str_0 = module_0.untokenize(list_0)
        untokenizer_0.add_whitespace(tuple_0)
        module_0.tokenize(callable_0)
    except BaseException:
        pass

def test_case_8():
    try:
        token_error_0 = module_0.TokenError()
        dict_0 = {}
        stop_tokenizing_0 = None
        grammar_0 = module_1.Grammar()
        iterator_0 = module_0.generate_tokens(stop_tokenizing_0, grammar_0)
        float_0 = -2077.0
        var_0 = module_0.printtoken(token_error_0, dict_0, iterator_0, float_0, stop_tokenizing_0)
    except BaseException:
        pass

def test_case_9():
    try:
        int_0 = 1
        str_0 = '?7rX\nQ9q>'
        tuple_0 = (int_0, str_0)
        iterable_0 = None
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.compat(tuple_0, iterable_0)
    except BaseException:
        pass