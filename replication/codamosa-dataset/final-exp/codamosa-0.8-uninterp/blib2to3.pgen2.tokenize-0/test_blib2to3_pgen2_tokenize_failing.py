# Automatically generated by Pynguin.
import blib2to3.pgen2.tokenize as module_0
import blib2to3.pgen2.grammar as module_1

def test_case_0():
    try:
        int_0 = None
        list_0 = [int_0, int_0]
        str_0 = '2'
        tuple_0 = None
        untokenizer_0 = module_0.Untokenizer()
        tuple_1 = (tuple_0, tuple_0)
        optional_0 = None
        iterator_0 = module_0.generate_tokens(tuple_1, optional_0)
        grammar_0 = None
        var_0 = module_0.printtoken(list_0, str_0, iterator_0, grammar_0, iterator_0)
    except BaseException:
        pass

def test_case_1():
    try:
        float_0 = 1151.839953
        module_0.tokenize(float_0)
    except BaseException:
        pass

def test_case_2():
    try:
        str_0 = '\x0bB'
        str_1 = module_0.untokenize(str_0)
    except BaseException:
        pass

def test_case_3():
    try:
        int_0 = 2
        str_0 = 'Mm/jLr86( nvE|J~/'
        tuple_0 = (int_0, str_0)
        grammar_0 = module_1.Grammar()
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.compat(tuple_0, grammar_0)
    except BaseException:
        pass

def test_case_4():
    try:
        int_0 = 15
        str_0 = 'un\'BB2r",fOM'
        tuple_0 = (int_0, str_0)
        bytes_0 = b'\x98}_\x87\x8a\x90\xee\x99\x1e\x8b\xaf\xa2\x85\x10'
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.compat(tuple_0, bytes_0)
    except BaseException:
        pass

def test_case_5():
    try:
        str_0 = 'I'
        tuple_0 = module_0.detect_encoding(str_0)
    except BaseException:
        pass

def test_case_6():
    try:
        int_0 = 2653
        tuple_0 = (int_0, int_0)
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.add_whitespace(tuple_0)
    except BaseException:
        pass

def test_case_7():
    try:
        str_0 = '79'
        str_1 = 'O6"'
        dict_0 = {str_0: str_0, str_0: str_1}
        untokenizer_0 = module_0.Untokenizer()
        str_2 = untokenizer_0.untokenize(dict_0)
        tuple_0 = None
        iterable_0 = None
        untokenizer_0.compat(tuple_0, iterable_0)
    except BaseException:
        pass

def test_case_8():
    try:
        str_0 = '2'
        tuple_0 = None
        untokenizer_0 = module_0.Untokenizer()
        tuple_1 = (tuple_0, tuple_0)
        optional_0 = None
        iterator_0 = module_0.generate_tokens(tuple_1, optional_0)
        grammar_0 = None
        dict_0 = {grammar_0: iterator_0, str_0: grammar_0}
        stop_tokenizing_0 = module_0.StopTokenizing()
        bytes_0 = b'\xa9\xea\xed\xeb\x9e\xb0\xb6\xb7[\xa9\xcb=\x0b\xcd\xc8\x89\xb6\x03\xf4'
        var_0 = module_0.printtoken(grammar_0, tuple_1, dict_0, stop_tokenizing_0, bytes_0)
    except BaseException:
        pass

def test_case_9():
    try:
        str_0 = 'def function(x):\n    return "result is " + repr(x)\n\nprint(function(23))\n'
        int_0 = -167
        tuple_0 = (int_0, str_0)
        grammar_0 = module_1.Grammar()
        untokenizer_0 = module_0.Untokenizer()
        list_0 = [tuple_0, grammar_0, tuple_0]
        untokenizer_0.compat(tuple_0, list_0)
    except BaseException:
        pass

def test_case_10():
    try:
        untokenizer_0 = module_0.Untokenizer()
        iterator_0 = None
        list_0 = [iterator_0, untokenizer_0]
        str_0 = '1##nS!\x0c66)i1[J1!2hEW'
        list_1 = None
        tuple_0 = (str_0, list_1)
        tuple_1 = ()
        var_0 = module_0.printtoken(iterator_0, untokenizer_0, list_0, tuple_0, tuple_1)
    except BaseException:
        pass

def test_case_11():
    try:
        str_0 = 'def function(x):\n    return "result is " + repr(x)\n\nprint(function(23))\n'
        int_0 = -179
        tuple_0 = (int_0, str_0)
        token_error_0 = module_0.TokenError()
        grammar_0 = module_1.Grammar()
        iterator_0 = module_0.generate_tokens(token_error_0, grammar_0)
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.compat(tuple_0, iterator_0)
    except BaseException:
        pass

def test_case_12():
    try:
        bytes_0 = b' \t\x0c\n\r'
        bytes_1 = [bytes_0]
        var_0 = iter(bytes_1)
        var_1 = var_0.__next__
        tuple_0 = module_0.detect_encoding(var_1)
        bytes_2 = [bytes_0, bytes_0]
        var_2 = iter(bytes_2)
        tuple_1 = module_0.detect_encoding(tuple_0)
    except BaseException:
        pass

def test_case_13():
    try:
        bytes_0 = b' \t\x0c\n\r'
        bytes_1 = [bytes_0]
        var_0 = iter(bytes_1)
        var_1 = var_0.__next__
        tuple_0 = module_0.detect_encoding(var_1)
        var_2 = iter(var_0)
        var_3 = var_2.__next__
        tuple_1 = module_0.detect_encoding(var_3)
        bytes_2 = b'\t\x0c\n\r#'
        bytes_3 = [bytes_2]
        stop_tokenizing_0 = module_0.StopTokenizing()
        var_4 = iter(bytes_3)
        var_5 = var_4.__next__
        str_0 = '"6vV"0Ieh A:Ayx'
        module_0.tokenize(str_0)
    except BaseException:
        pass

def test_case_14():
    try:
        str_0 = 'pass'
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        var_2 = module_0.tokenize_loop(var_1, str_0)
    except BaseException:
        pass

def test_case_15():
    try:
        str_0 = 'print1)\n'
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        iterator_0 = module_0.generate_tokens(var_1)
        var_2 = [x for x in iterator_0]
    except BaseException:
        pass

def test_case_16():
    try:
        str_0 = '$s'
        str_1 = [str_0, str_0, str_0]
        var_0 = iter(str_1)
        var_1 = var_0.__next__
        var_2 = None
        var_3 = module_0.tokenize_loop(var_1, var_2)
    except BaseException:
        pass

def test_case_17():
    try:
        str_0 = 'if (21: #comment\n'
        str_1 = [str_0, str_0]
        var_0 = iter(str_1)
        var_1 = var_0.__next__
        iterator_0 = module_0.generate_tokens(var_1)
        var_2 = [x for x in iterator_0]
    except BaseException:
        pass

def test_case_18():
    try:
        str_0 = '\t~f<vCi^\x0c"0F8'
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        var_2 = module_0.tokenize_loop(var_1, var_1)
    except BaseException:
        pass

def test_case_19():
    try:
        str_0 = '\n        Initializer.  Takes optional type, content, and name.\n\n        The type, if given must be a token type (< 256).  If not given,\n        this matches any *leaf* node; the content may still be required.\n\n        The content, if given, must be a string.\n\n        If a name is given, the matching node is stored in the results\n        dict under that key.\n        '
        str_1 = '  print(1)\n'
        str_2 = [str_0, str_1]
        var_0 = iter(str_2)
        var_1 = var_0.__next__
        iterator_0 = module_0.generate_tokens(var_1)
        var_2 = [x for x in iterator_0]
        grammar_0 = module_1.Grammar()
        tuple_0 = module_0.detect_encoding(grammar_0)
    except BaseException:
        pass

def test_case_20():
    try:
        str_0 = 'if 1: #comment\n'
        str_1 = 'ahyw\t*(A?r&m'
        str_2 = [str_0, str_1]
        var_0 = iter(str_2)
        var_1 = var_0.__next__
        iterator_0 = module_0.generate_tokens(var_1)
        var_2 = [x for x in iterator_0]
    except BaseException:
        pass

def test_case_21():
    try:
        str_0 = "'dblq.0\x0b.dqlme7"
        str_1 = 'ahyw\t*(A?r&m'
        str_2 = [str_0, str_1]
        var_0 = iter(str_2)
        var_1 = var_0.__next__
        iterator_0 = module_0.generate_tokens(var_1)
        var_2 = [x for x in iterator_0]
    except BaseException:
        pass

def test_case_22():
    try:
        str_0 = "'"
        list_0 = []
        tuple_0 = (str_0, list_0)
        untokenizer_0 = module_0.Untokenizer()
        list_1 = [tuple_0]
        int_0 = False
        tuple_1 = (int_0, int_0)
        untokenizer_0.add_whitespace(tuple_1)
        var_0 = module_0.any(*list_1)
    except BaseException:
        pass