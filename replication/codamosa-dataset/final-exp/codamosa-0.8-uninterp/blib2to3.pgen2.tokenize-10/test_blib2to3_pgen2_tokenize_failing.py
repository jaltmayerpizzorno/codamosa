# Automatically generated by Pynguin.
import blib2to3.pgen2.grammar as module_0
import blib2to3.pgen2.tokenize as module_1

def test_case_0():
    try:
        grammar_0 = module_0.Grammar()
        list_0 = []
        list_1 = [list_0, list_0, grammar_0]
        callable_0 = None
        iterator_0 = module_1.generate_tokens(callable_0)
        int_0 = -293
        dict_0 = {}
        var_0 = module_1.printtoken(grammar_0, list_1, iterator_0, int_0, dict_0)
    except BaseException:
        pass

def test_case_1():
    try:
        int_0 = -793
        var_0 = module_1.group()
        str_0 = 'D^[}\nIxF<1'
        tuple_0 = (int_0, str_0)
        str_1 = ')m7|Hn\tT2T'
        dict_0 = {str_0: tuple_0, str_1: str_0}
        float_0 = None
        grammar_0 = module_0.Grammar()
        var_1 = grammar_0.copy()
        list_0 = []
        token_error_0 = module_1.TokenError(*list_0)
        tuple_1 = ()
        var_2 = module_1.printtoken(float_0, var_1, dict_0, token_error_0, tuple_1)
    except BaseException:
        pass

def test_case_2():
    try:
        callable_0 = None
        module_1.tokenize(callable_0)
    except BaseException:
        pass

def test_case_3():
    try:
        str_0 = "Initializer.\n\n        Creates an attribute for each grammar symbol (nonterminal),\n        whose value is the symbol's type (an int >= 256).\n        "
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        iterator_0 = module_1.generate_tokens(var_1)
        untokenizer_0 = module_1.Untokenizer()
        str_1 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass

def test_case_4():
    try:
        complex_0 = None
        tuple_0 = (complex_0,)
        untokenizer_0 = module_1.Untokenizer()
        int_0 = -1008
        tuple_1 = (int_0, int_0)
        untokenizer_0.add_whitespace(tuple_1)
        tuple_2 = module_1.detect_encoding(tuple_0)
    except BaseException:
        pass

def test_case_5():
    try:
        str_0 = 'n+5_&'
        dict_0 = None
        dict_1 = {str_0: dict_0, str_0: str_0, str_0: str_0, str_0: str_0}
        untokenizer_0 = module_1.Untokenizer()
        str_1 = untokenizer_0.untokenize(dict_1)
    except BaseException:
        pass

def test_case_6():
    try:
        int_0 = 3490
        str_0 = module_1.untokenize(int_0)
    except BaseException:
        pass

def test_case_7():
    try:
        str_0 = 'el&'
        bytes_0 = b'8\xb3\x9b\x80\x92xHg\xdcf\xd1\xd4\xb2/?\x9f'
        bytes_1 = b'az\xef'
        list_0 = [bytes_0, bytes_1]
        tuple_0 = (str_0, list_0)
        str_1 = module_1.untokenize(tuple_0)
    except BaseException:
        pass

def test_case_8():
    try:
        untokenizer_0 = module_1.Untokenizer()
        int_0 = 8
        str_0 = None
        tuple_0 = (int_0, str_0)
        bool_0 = False
        untokenizer_0.compat(tuple_0, bool_0)
    except BaseException:
        pass

def test_case_9():
    try:
        callable_0 = None
        tuple_0 = module_1.detect_encoding(callable_0)
    except BaseException:
        pass

def test_case_10():
    try:
        str_0 = '"'
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        iterator_0 = module_1.generate_tokens(var_1)
        untokenizer_0 = module_1.Untokenizer()
        str_1 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass

def test_case_11():
    try:
        str_0 = "'''"
        str_1 = '"'
        var_0 = str_0 + str_1
        var_1 = [str_0, str_0, str_0, var_0, str_1]
        var_2 = iter(var_1)
        var_3 = var_2.__next__
        iterator_0 = module_1.generate_tokens(var_3)
        untokenizer_0 = module_1.Untokenizer()
        str_2 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass

def test_case_12():
    try:
        str_0 = "'''"
        str_1 = '   123'
        str_2 = '"'
        var_0 = str_1 + str_2
        var_1 = [str_0, str_1, str_1, var_0, str_2]
        var_2 = iter(var_1)
        var_3 = var_2.__next__
        iterator_0 = module_1.generate_tokens(var_3)
        untokenizer_0 = module_1.Untokenizer()
        str_3 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass

def test_case_13():
    try:
        str_0 = '4M'
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        iterator_0 = module_1.generate_tokens(var_1)
        untokenizer_0 = module_1.Untokenizer()
        str_1 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass

def test_case_14():
    try:
        str_0 = '\tj{/).#nW\n1O#Yt'
        var_0 = [str_0, str_0, str_0, str_0, str_0]
        var_1 = iter(var_0)
        var_2 = var_1.__next__
        iterator_0 = module_1.generate_tokens(var_2)
        untokenizer_0 = module_1.Untokenizer()
        str_1 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass

def test_case_15():
    try:
        str_0 = '4M'
        var_0 = [str_0, str_0, str_0, str_0, str_0]
        var_1 = iter(var_0)
        var_2 = var_1.__next__
        iterator_0 = module_1.generate_tokens(var_2)
        untokenizer_0 = module_1.Untokenizer()
        str_1 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass

def test_case_16():
    try:
        str_0 = "gP@S 6'b\t<gO,)`<\rX6'"
        var_0 = [str_0, str_0, str_0, str_0, str_0]
        var_1 = iter(var_0)
        var_2 = var_1.__next__
        iterator_0 = module_1.generate_tokens(var_2)
        untokenizer_0 = module_1.Untokenizer()
        str_1 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass

def test_case_17():
    try:
        str_0 = "'[''"
        str_1 = '   123'
        str_2 = '"'
        var_0 = str_0 + str_2
        var_1 = [str_0, str_1, str_1, var_0, str_2]
        var_2 = iter(var_1)
        var_3 = var_2.__next__
        str_3 = 'UyyPVC'
        iterator_0 = module_1.generate_tokens(str_3)
        callable_0 = None
        grammar_0 = module_0.Grammar()
        iterator_1 = module_1.generate_tokens(callable_0, grammar_0)
        untokenizer_0 = module_1.Untokenizer()
        str_4 = untokenizer_0.untokenize(iterator_1)
    except BaseException:
        pass

def test_case_18():
    try:
        str_0 = '   123%O'
        str_1 = '"'
        var_0 = [str_0, str_1, str_0, str_0, str_1]
        var_1 = iter(var_0)
        var_2 = var_1.__next__
        iterator_0 = module_1.generate_tokens(var_2)
        untokenizer_0 = module_1.Untokenizer()
        str_2 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass

def test_case_19():
    try:
        str_0 = '   123-%O(O'
        str_1 = 'M'
        var_0 = [str_0, str_1, str_0, str_0, str_1]
        var_1 = iter(var_0)
        var_2 = var_1.__next__
        iterator_0 = module_1.generate_tokens(var_2)
        untokenizer_0 = module_1.Untokenizer()
        str_2 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass

def test_case_20():
    try:
        str_0 = '   123%O'
        str_1 = ''
        var_0 = str_1 + str_0
        var_1 = [str_0, str_1, str_0, var_0, str_1]
        var_2 = iter(var_1)
        var_3 = var_2.__next__
        iterator_0 = module_1.generate_tokens(var_3)
        untokenizer_0 = module_1.Untokenizer()
        str_2 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass

def test_case_21():
    try:
        str_0 = '\rCO16'
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        iterator_0 = module_1.generate_tokens(var_1)
        untokenizer_0 = module_1.Untokenizer()
        str_1 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass

def test_case_22():
    try:
        str_0 = '#@m?!pt['
        str_1 = '"'
        var_0 = str_0 + str_1
        var_1 = [str_0, str_1, str_0, var_0, str_1]
        var_2 = iter(var_1)
        var_3 = var_2.__next__
        iterator_0 = module_1.generate_tokens(var_3)
        untokenizer_0 = module_1.Untokenizer()
        str_2 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass

def test_case_23():
    try:
        str_0 = "'''"
        str_1 = '   123'
        str_2 = '  '
        str_3 = '"'
        int_0 = 1
        tuple_0 = (int_0, str_1)
        int_1 = 1730
        str_4 = '0hs?'
        str_5 = 'i^c"qdaUW&\t'
        dict_0 = {str_3: str_0, str_2: str_1, str_4: str_4, str_5: int_1}
        tuple_1 = (int_1, dict_0)
        untokenizer_0 = module_1.Untokenizer()
        untokenizer_0.compat(tuple_0, tuple_1)
    except BaseException:
        pass

def test_case_24():
    try:
        str_0 = ' (  #23%O'
        str_1 = ''
        var_0 = str_0 + str_1
        var_1 = [str_0, str_1, str_0, var_0, str_1]
        var_2 = iter(var_1)
        var_3 = var_2.__next__
        iterator_0 = module_1.generate_tokens(var_3)
        untokenizer_0 = module_1.Untokenizer()
        str_2 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass

def test_case_25():
    try:
        list_0 = []
        var_0 = module_1.any(*list_0)
        str_0 = '\tj{/.#nW\n#t'
        var_1 = [str_0, str_0, str_0, str_0, str_0]
        var_2 = iter(var_1)
        var_3 = var_2.__next__
        stop_tokenizing_0 = module_1.StopTokenizing()
        iterator_0 = module_1.generate_tokens(var_3)
        untokenizer_0 = module_1.Untokenizer()
        str_1 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass

def test_case_26():
    try:
        str_0 = ' abc'
        str_1 = '   123%O'
        str_2 = '"'
        var_0 = str_0 + str_0
        var_1 = module_1.maybe()
        var_2 = [str_1, str_0, str_0, var_0, str_2]
        var_3 = iter(var_2)
        var_4 = var_3.__next__
        iterator_0 = module_1.generate_tokens(var_4)
        untokenizer_0 = module_1.Untokenizer()
        str_3 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass