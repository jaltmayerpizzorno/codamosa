# Automatically generated by Pynguin.
import blib2to3.pgen2.tokenize as module_0
import blib2to3.pgen2.grammar as module_1

def test_case_0():
    try:
        untokenizer_0 = module_0.Untokenizer()
        grammar_0 = module_1.Grammar()
        iterator_0 = module_0.generate_tokens(untokenizer_0, grammar_0)
        iterator_1 = module_0.generate_tokens(iterator_0, grammar_0)
        stop_tokenizing_0 = module_0.StopTokenizing()
        list_0 = [grammar_0, stop_tokenizing_0]
        var_0 = grammar_0.copy()
        str_0 = '0'
        list_1 = None
        tuple_0 = (str_0, list_1)
        var_1 = module_0.printtoken(iterator_1, stop_tokenizing_0, list_0, var_0, tuple_0)
    except BaseException:
        pass

def test_case_1():
    try:
        callable_0 = None
        module_0.tokenize(callable_0)
    except BaseException:
        pass

def test_case_2():
    try:
        bool_0 = True
        set_0 = {bool_0, bool_0, bool_0}
        str_0 = module_0.untokenize(set_0)
    except BaseException:
        pass

def test_case_3():
    try:
        bool_0 = True
        tuple_0 = module_0.detect_encoding(bool_0)
    except BaseException:
        pass

def test_case_4():
    try:
        callable_0 = None
        untokenizer_0 = module_0.Untokenizer()
        iterator_0 = module_0.generate_tokens(callable_0)
        int_0 = 798
        int_1 = 234
        tuple_0 = (int_0, int_1)
        untokenizer_0.add_whitespace(tuple_0)
    except BaseException:
        pass

def test_case_5():
    try:
        str_0 = 'oIx'
        untokenizer_0 = module_0.Untokenizer()
        str_1 = untokenizer_0.untokenize(str_0)
    except BaseException:
        pass

def test_case_6():
    try:
        untokenizer_0 = module_0.Untokenizer()
        int_0 = 52
        str_0 = "'MHvV:W&~qh-\\C"
        tuple_0 = (int_0, str_0)
        token_error_0 = module_0.TokenError()
        iterator_0 = module_0.generate_tokens(token_error_0)
        iterator_1 = module_0.generate_tokens(iterator_0, iterator_0)
        untokenizer_0.compat(tuple_0, iterator_1)
    except BaseException:
        pass

def test_case_7():
    try:
        callable_0 = None
        untokenizer_0 = module_0.Untokenizer()
        list_0 = []
        str_0 = untokenizer_0.untokenize(list_0)
        module_0.tokenize(callable_0)
    except BaseException:
        pass

def test_case_8():
    try:
        int_0 = 1
        str_0 = "@>2@c3yb;7/0A,,-'"
        tuple_0 = (int_0, str_0)
        iterable_0 = None
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.compat(tuple_0, iterable_0)
    except BaseException:
        pass

def test_case_9():
    try:
        token_error_0 = module_0.TokenError()
        stop_tokenizing_0 = module_0.StopTokenizing()
        str_0 = 'B9gc3m[@qg>AX'
        dict_0 = {stop_tokenizing_0: token_error_0, str_0: token_error_0}
        str_1 = None
        list_0 = []
        tuple_0 = (str_1, list_0)
        bool_0 = True
        var_0 = module_0.printtoken(token_error_0, stop_tokenizing_0, dict_0, tuple_0, bool_0)
    except BaseException:
        pass

def test_case_10():
    try:
        int_0 = 544
        str_0 = '?&XLDgJe</F*[g>d%'
        tuple_0 = (int_0, str_0)
        set_0 = {tuple_0}
        untokenizer_0 = module_0.Untokenizer()
        str_1 = module_0.untokenize(set_0)
        callable_0 = None
        iterator_0 = module_0.generate_tokens(callable_0)
        str_2 = "Y'D,j"
        str_3 = ',\t1&B3lX\x0b{~\rW'
        dict_0 = {str_2: int_0, str_0: str_0, str_3: str_2, str_0: int_0}
        str_4 = untokenizer_0.untokenize(dict_0)
    except BaseException:
        pass

def test_case_11():
    try:
        int_0 = 4
        str_0 = 'UZ?#Y'
        tuple_0 = (int_0, str_0)
        grammar_0 = module_1.Grammar()
        var_0 = grammar_0.copy()
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.compat(tuple_0, var_0)
    except BaseException:
        pass

def test_case_12():
    try:
        int_0 = 56
        str_0 = '?Y!&XDgJe</F*[g>d%'
        tuple_0 = (int_0, str_0)
        set_0 = {tuple_0}
        untokenizer_0 = module_0.Untokenizer()
        callable_0 = None
        iterator_0 = module_0.generate_tokens(callable_0)
        str_1 = untokenizer_0.untokenize(set_0)
        token_error_0 = module_0.TokenError()
        grammar_0 = module_1.Grammar()
        module_0.tokenize(callable_0)
    except BaseException:
        pass

def test_case_13():
    try:
        var_0 = module_0.any()
        var_1 = iter(var_0)
        var_2 = var_1.__next__
        var_3 = module_0.tokenize_loop(var_2, var_0)
    except BaseException:
        pass

def test_case_14():
    try:
        untokenizer_0 = module_0.Untokenizer()
        int_0 = 3
        str_0 = ''
        int_1 = 1
        var_0 = (int_1, str_0)
        var_1 = (int_0, str_0)
        var_2 = [var_0, var_1]
        str_1 = untokenizer_0.untokenize(var_2)
        int_2 = 0
        var_3 = (int_2, str_0)
        str_2 = 'def'
        var_4 = (int_1, str_2)
        int_3 = 4
        str_3 = 'ghi'
        var_5 = (int_3, str_3)
        str_4 = 'jkl'
        var_6 = (int_2, str_4)
        var_7 = [var_3, var_4, var_5, var_6]
        str_5 = untokenizer_0.untokenize(var_7)
        var_8 = (int_2, str_3)
        var_9 = [var_7, var_8]
        str_6 = untokenizer_0.untokenize(var_9)
    except BaseException:
        pass

def test_case_15():
    try:
        str_0 = 'o!\n'
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        var_2 = module_0.tokenize_loop(var_1, var_1)
    except BaseException:
        pass

def test_case_16():
    try:
        str_0 = ',!\n'
        var_0 = lambda ttype, token, *args: tokens.append((ttype, token))
        var_1 = iter(str_0)
        var_2 = var_1.__next__
        var_3 = module_0.tokenize_loop(var_2, var_0)
    except BaseException:
        pass

def test_case_17():
    try:
        str_0 = ''
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        var_2 = module_0.tokenize_loop(var_1, var_0)
    except BaseException:
        pass

def test_case_18():
    try:
        str_0 = '\r'
        int_0 = -2750
        tuple_0 = (int_0, int_0)
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.add_whitespace(tuple_0)
        var_0 = lambda ttype, token, *args: tokens.append((ttype, token))
        var_1 = iter(str_0)
        untokenizer_1 = module_0.Untokenizer()
        var_2 = var_1.__next__
        var_3 = module_0.tokenize_loop(var_2, var_0)
    except BaseException:
        pass

def test_case_19():
    try:
        str_0 = '#KuWwOj~stxqn-'
        int_0 = -2750
        tuple_0 = (int_0, int_0)
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.add_whitespace(tuple_0)
        var_0 = lambda ttype, token, *args: tokens.append((ttype, token))
        var_1 = iter(str_0)
        untokenizer_1 = module_0.Untokenizer()
        var_2 = var_1.__next__
        var_3 = module_0.tokenize_loop(var_2, var_0)
    except BaseException:
        pass

def test_case_20():
    try:
        list_0 = []
        dict_0 = {}
        token_error_0 = module_0.TokenError(*list_0, **dict_0)
        str_0 = ']c3#~oUB=f'
        var_0 = []
        var_1 = iter(str_0)
        untokenizer_0 = module_0.Untokenizer()
        var_2 = var_1.__next__
        var_3 = module_0.tokenize_loop(var_2, var_0)
    except BaseException:
        pass

def test_case_21():
    try:
        str_0 = '\t\n{'
        var_0 = lambda ttype, token, *args: tokens.append((ttype, token))
        var_1 = iter(str_0)
        var_2 = module_0.maybe()
        untokenizer_0 = module_0.Untokenizer()
        var_3 = var_1.__next__
        var_4 = module_0.tokenize_loop(var_3, var_0)
    except BaseException:
        pass

def test_case_22():
    try:
        str_0 = '\x0c-'
        int_0 = -2750
        untokenizer_0 = module_0.Untokenizer()
        tuple_0 = (int_0, int_0)
        untokenizer_1 = module_0.Untokenizer()
        var_0 = iter(str_0)
        untokenizer_2 = module_0.Untokenizer()
        var_1 = module_0.maybe()
        untokenizer_2.add_whitespace(tuple_0)
        var_2 = var_0.__next__
        var_3 = module_0.tokenize_loop(var_2, int_0)
    except BaseException:
        pass

def test_case_23():
    try:
        untokenizer_0 = module_0.Untokenizer()
        str_0 = '\x0b\ram?DTjd\rpgl.Y'
        int_0 = -2678
        tuple_0 = (int_0, int_0)
        tuple_1 = (int_0, str_0)
        dict_0 = {}
        untokenizer_0.compat(tuple_1, dict_0)
        untokenizer_1 = module_0.Untokenizer()
        untokenizer_1.add_whitespace(tuple_0)
        var_0 = lambda ttype, token, *args: tokens.append((ttype, token))
        var_1 = iter(str_0)
        untokenizer_2 = module_0.Untokenizer()
        var_2 = var_1.__next__
        var_3 = module_0.tokenize_loop(var_2, var_0)
    except BaseException:
        pass