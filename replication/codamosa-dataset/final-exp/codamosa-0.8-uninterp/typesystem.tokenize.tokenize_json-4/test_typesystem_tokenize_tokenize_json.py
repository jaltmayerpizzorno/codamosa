# Automatically generated by Pynguin.
import typesystem.tokenize.tokenize_json as module_0

def test_case_0():
    pass

def test_case_1():
    str_0 = '\n    {\n        "first": "foo",\n       "second": [\n            1, 2, 3\n        ],\n        "third": {\n            "child1": true,\n            "child2": false\n        }\n   }\n    '
    token_0 = module_0.tokenize_json(str_0)

def test_case_2():
    str_0 = 'null'
    token_0 = module_0.tokenize_json(str_0)

def test_case_3():
    str_0 = '{"x": 1, "y": 2}'
    token_0 = module_0.tokenize_json(str_0)

def test_case_4():
    str_0 = '\n    {\n        "first": "foo",\n        "second": [\n            1, 2, 3\n        ],\n        "third": {\n            "child1": true,\n            "child2":false\n        }\n    }\n    '
    token_0 = module_0.tokenize_json(str_0)

def test_case_5():
    str_0 = '{"key1": "val1", "key2": "val2"}'
    token_0 = module_0.tokenize_json(str_0)
    str_1 = '{}'
    token_1 = module_0.tokenize_json(str_1)
    str_2 = '[]'
    token_2 = module_0.tokenize_json(str_2)
    str_3 = '1'
    token_3 = module_0.tokenize_json(str_3)
    str_4 = 'true'
    token_4 = module_0.tokenize_json(str_4)
    token_5 = module_0.tokenize_json(str_2)
    token_6 = module_0.tokenize_json(str_1)