# Automatically generated by Pynguin.
import blib2to3.pgen2.tokenize as module_0
import blib2to3.pgen2.grammar as module_1

def test_case_0():
    pass

def test_case_1():
    untokenizer_0 = module_0.Untokenizer()

def test_case_2():
    int_0 = -889
    tuple_0 = (int_0, int_0)
    untokenizer_0 = module_0.Untokenizer()
    untokenizer_0.add_whitespace(tuple_0)

def test_case_3():
    grammar_0 = module_1.Grammar()
    list_0 = []
    grammar_1 = module_1.Grammar()
    callable_0 = None
    iterator_0 = module_0.generate_tokens(callable_0)
    untokenizer_0 = module_0.Untokenizer()
    str_0 = untokenizer_0.untokenize(list_0)
    stop_tokenizing_0 = module_0.StopTokenizing()

def test_case_4():
    untokenizer_0 = module_0.Untokenizer()
    int_0 = None
    str_0 = '@w9h{BZl'
    tuple_0 = (int_0, str_0)
    str_1 = ':Y'
    bytes_0 = b'_*D\x1e\xe95Um-\x18\xf7m/'
    bytes_1 = b'\xdeU\x9e\xcb\xb1\xd3\xf6\xe7\xeb\xe8\xdf\xc1\xa8\x948'
    list_0 = [bytes_0, bytes_1, bytes_0]
    tuple_1 = (str_1, list_0)
    untokenizer_0.compat(tuple_0, tuple_1)

def test_case_5():
    untokenizer_0 = module_0.Untokenizer()
    var_0 = module_0.group()
    list_0 = [var_0, var_0, var_0, var_0]
    untokenizer_1 = module_0.Untokenizer()
    var_1 = module_0.maybe()
    str_0 = module_0.untokenize(list_0)
    int_0 = -674
    int_1 = -903
    tuple_0 = (int_0, int_1)
    untokenizer_2 = module_0.Untokenizer()
    untokenizer_2.add_whitespace(tuple_0)
    dict_0 = {}
    token_error_0 = module_0.TokenError(**dict_0)