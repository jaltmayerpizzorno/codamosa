# Automatically generated by Pynguin.
import typesystem.tokenize.tokenize_json as module_0

def test_case_0():
    str_0 = '{"foo": "bar", "baz": null, "qux": true, "quux": false, "corge": 1.2, "grault": -9876, "garple": -0, "waldo": [1,2,"3",[4,[5]]]}'
    token_0 = module_0.tokenize_json(str_0)

def test_case_1():
    str_0 = '{"a": "A", "b": {"c": "C"}}'
    token_0 = module_0.tokenize_json(str_0)

def test_case_2():
    str_0 = '\n    [{\n        "first_name": "John",\n        "last_name": "Doe",\n        "age": 42,\n        "male": true\n    },\n    {\n        "first_name": "Jane",\n        "last_name": "Doe",\n        "age": 38,\n        "male": false\n    }]\n    '
    token_0 = module_0.tokenize_json(str_0)

def test_case_3():
    str_0 = '{"A": 1, "B": ["a", "b"], "C": {"a": "b"}, "D": 1.1, "F": null}'
    token_0 = module_0.tokenize_json(str_0)

def test_case_4():
    str_0 = '{ "a" : 2 }'
    token_0 = module_0.tokenize_json(str_0)

def test_case_5():
    str_0 = '{"key": "value"}'
    token_0 = module_0.tokenize_json(str_0)
    var_0 = type(token_0)
    str_1 = '["one",2,{"three":[4,5]}]'
    token_1 = module_0.tokenize_json(str_1)
    var_1 = type(token_1)
    str_2 = '{"key":null}'
    token_2 = module_0.tokenize_json(str_2)
    var_2 = type(token_2)
    str_3 = '{"key":true}'
    token_3 = module_0.tokenize_json(str_3)
    var_3 = type(token_3)
    str_4 = '{"key":1.5}'
    token_4 = module_0.tokenize_json(str_4)
    var_4 = type(token_4)
    str_5 = '{"key":1e4}'
    token_5 = module_0.tokenize_json(str_5)
    var_5 = type(token_5)
    str_6 = '{"key":[]}'
    token_6 = module_0.tokenize_json(str_6)
    var_6 = type(token_6)
    str_7 = '{"key":{}}'
    token_7 = module_0.tokenize_json(str_7)
    var_7 = type(token_7)