# Automatically generated by Pynguin.
import blib2to3.pgen2.grammar as module_0
import blib2to3.pgen2.tokenize as module_1

def test_case_0():
    try:
        str_0 = 'Zs2V<;D-Kks3o`xJu'
        str_1 = 'Sl9;fi4)Nq'
        str_2 = '*rd2z1U>aw4)'
        dict_0 = {str_0: str_0, str_1: str_1, str_2: str_2, str_0: str_0}
        grammar_0 = module_0.Grammar()
        untokenizer_0 = module_1.Untokenizer()
        str_3 = '\x0bO_'
        bool_0 = True
        int_0 = 1884
        tuple_0 = (str_3, bool_0, int_0, dict_0)
        var_0 = module_1.printtoken(dict_0, grammar_0, str_2, untokenizer_0, tuple_0)
    except BaseException:
        pass

def test_case_1():
    try:
        callable_0 = None
        module_1.tokenize(callable_0)
    except BaseException:
        pass

def test_case_2():
    try:
        untokenizer_0 = module_1.Untokenizer()
        str_0 = ']@$jb\t'
        str_1 = untokenizer_0.untokenize(str_0)
    except BaseException:
        pass

def test_case_3():
    try:
        int_0 = -666
        str_0 = '%<'
        tuple_0 = (int_0, str_0)
        iterable_0 = None
        untokenizer_0 = module_1.Untokenizer()
        untokenizer_0.compat(tuple_0, iterable_0)
    except BaseException:
        pass

def test_case_4():
    try:
        callable_0 = None
        tuple_0 = module_1.detect_encoding(callable_0)
    except BaseException:
        pass

def test_case_5():
    try:
        bytes_0 = b'\xf4/\xc0t\x9dc\x96\xc2\x12c\xd5|l\xcaI&!7'
        list_0 = [bytes_0, bytes_0, bytes_0]
        str_0 = 'KkOBf\t\\d>d'
        str_1 = 'y>k\t1w\r`537~9rv'
        dict_0 = {str_0: str_1, str_1: str_1}
        tuple_0 = (list_0, str_1, dict_0, dict_0)
        str_2 = module_1.untokenize(tuple_0)
    except BaseException:
        pass

def test_case_6():
    try:
        var_0 = module_1.maybe()
        untokenizer_0 = module_1.Untokenizer()
        int_0 = 232
        tuple_0 = (int_0, int_0)
        untokenizer_0.add_whitespace(tuple_0)
    except BaseException:
        pass

def test_case_7():
    try:
        int_0 = 4
        stop_tokenizing_0 = module_1.StopTokenizing()
        str_0 = '\tIvJp!&3}sVe\tJ'
        tuple_0 = (int_0, str_0)
        dict_0 = {str_0: int_0, str_0: tuple_0}
        untokenizer_0 = module_1.Untokenizer()
        untokenizer_0.compat(tuple_0, dict_0)
        int_1 = -131
        tuple_1 = (int_1, int_1)
        untokenizer_0.add_whitespace(tuple_1)
        stop_tokenizing_1 = module_1.StopTokenizing()
        iterable_0 = None
        untokenizer_1 = module_1.Untokenizer()
        untokenizer_1.compat(tuple_0, iterable_0)
    except BaseException:
        pass

def test_case_8():
    try:
        int_0 = -156
        str_0 = '!Rhk}'
        tuple_0 = (int_0, str_0)
        list_0 = []
        tuple_1 = (str_0, list_0)
        untokenizer_0 = module_1.Untokenizer()
        untokenizer_0.compat(tuple_0, tuple_1)
    except BaseException:
        pass

def test_case_9():
    try:
        int_0 = -1269
        str_0 = '\tIvJp!&3}sVe\tJ'
        tuple_0 = (int_0, str_0)
        dict_0 = {}
        untokenizer_0 = module_1.Untokenizer()
        untokenizer_0.compat(tuple_0, dict_0)
        var_0 = module_1.any()
        untokenizer_1 = module_1.Untokenizer()
        tuple_1 = (int_0, int_0)
        untokenizer_2 = module_1.Untokenizer()
        untokenizer_2.add_whitespace(tuple_1)
        callable_0 = None
        grammar_0 = module_0.Grammar()
        iterator_0 = module_1.generate_tokens(callable_0, grammar_0)
        untokenizer_2.compat(tuple_0, iterator_0)
    except BaseException:
        pass

def test_case_10():
    try:
        str_0 = 'U}.`5'
        list_0 = [str_0, str_0, str_0]
        str_1 = 'q!NOnn5LV1h-J{'
        bytes_0 = b'\x86\xe9\x9e\xfb:\x83<\x13\xcd\xa4F\xf6\x11^'
        bytes_1 = b'\x00\xb7\x15\xa5.\x07\xca'
        bytes_2 = b'\x9eu\xdc@a'
        list_1 = [bytes_0, bytes_1, bytes_2, bytes_2]
        tuple_0 = (str_1, list_1)
        str_2 = 'i|'
        bool_0 = False
        var_0 = module_1.printtoken(list_0, str_1, tuple_0, str_2, bool_0)
    except BaseException:
        pass

def test_case_11():
    try:
        untokenizer_0 = module_1.Untokenizer()
        str_0 = 'NL'
        int_0 = -1538
        int_1 = 2720
        tuple_0 = (int_0, int_1)
        untokenizer_0.add_whitespace(tuple_0)
        str_1 = '\n'
        str_2 = (str_0, str_1)
        str_3 = untokenizer_0.untokenize(str_2)
    except BaseException:
        pass

def test_case_12():
    try:
        untokenizer_0 = module_1.Untokenizer()
        str_0 = '#test'
        int_0 = -1538
        int_1 = 2720
        tuple_0 = (int_0, int_1)
        untokenizer_0.add_whitespace(tuple_0)
        str_1 = '\n'
        str_2 = (str_0, str_1)
        str_3 = untokenizer_0.untokenize(str_2)
    except BaseException:
        pass

def test_case_13():
    try:
        int_0 = 4
        stop_tokenizing_0 = module_1.StopTokenizing()
        str_0 = '\tIvJp!&3}sVe\tJ'
        tuple_0 = (int_0, str_0)
        dict_0 = {str_0: int_0, str_0: tuple_0}
        untokenizer_0 = module_1.Untokenizer()
        untokenizer_0.compat(tuple_0, dict_0)
        bytes_0 = b'\x7fP*\xa0L\x90x '
        iterator_0 = module_1.generate_tokens(untokenizer_0)
        iterator_1 = module_1.generate_tokens(iterator_0)
        bytes_1 = b''
        var_0 = module_1.any()
        bytes_2 = b'|\xe2Rgd%\xcd\xd6\xa3v'
        list_0 = [bytes_0, bytes_1, bytes_2]
        int_1 = -131
        tuple_1 = (int_1, int_1)
        untokenizer_1 = module_1.Untokenizer()
        untokenizer_1.add_whitespace(tuple_1)
        token_error_0 = module_1.TokenError()
        stop_tokenizing_1 = module_1.StopTokenizing()
        tuple_2 = (str_0, list_0)
        var_1 = module_1.maybe()
        stop_tokenizing_2 = module_1.StopTokenizing()
        list_1 = [tuple_2, str_0]
        str_1 = untokenizer_1.untokenize(list_1)
    except BaseException:
        pass