# Automatically generated by Pynguin.
import blib2to3.pgen2.grammar as module_0
import blib2to3.pgen2.tokenize as module_1

def test_case_0():
    try:
        grammar_0 = module_0.Grammar()
        var_0 = grammar_0.copy()
        iterator_0 = module_1.generate_tokens(var_0)
        bool_0 = True
        str_0 = ''
        str_1 = 'O'
        dict_0 = {str_1: str_1}
        var_1 = module_1.printtoken(iterator_0, bool_0, str_0, dict_0, dict_0)
    except BaseException:
        pass

def test_case_1():
    try:
        callable_0 = None
        module_1.tokenize(callable_0)
    except BaseException:
        pass

def test_case_2():
    try:
        int_0 = 713
        int_1 = 0
        tuple_0 = (int_0, int_1)
        untokenizer_0 = module_1.Untokenizer()
        untokenizer_0.add_whitespace(tuple_0)
    except BaseException:
        pass

def test_case_3():
    try:
        callable_0 = None
        str_0 = '%ys?KbQpT/\nOlBL\x0b`z=!'
        dict_0 = {str_0: callable_0, str_0: callable_0}
        str_1 = module_1.untokenize(dict_0)
    except BaseException:
        pass

def test_case_4():
    try:
        tuple_0 = ()
        str_0 = module_1.untokenize(tuple_0)
        int_0 = 9
        tuple_1 = (int_0, str_0)
        float_0 = 1712.025045
        untokenizer_0 = module_1.Untokenizer()
        untokenizer_0.compat(tuple_1, float_0)
    except BaseException:
        pass

def test_case_5():
    try:
        int_0 = 9
        str_0 = 'c|'
        tuple_0 = (int_0, str_0)
        float_0 = 1712.025045
        untokenizer_0 = module_1.Untokenizer()
        untokenizer_0.compat(tuple_0, float_0)
    except BaseException:
        pass

def test_case_6():
    try:
        callable_0 = None
        tuple_0 = module_1.detect_encoding(callable_0)
    except BaseException:
        pass

def test_case_7():
    try:
        int_0 = 3413
        grammar_0 = module_0.Grammar()
        set_0 = {int_0, int_0}
        token_error_0 = module_1.TokenError()
        var_0 = module_1.any()
        str_0 = 'W7'
        iterator_0 = module_1.generate_tokens(int_0)
        bytes_0 = b'#\xf7'
        str_1 = '.Q'
        var_1 = module_1.printtoken(set_0, token_error_0, str_0, bytes_0, str_1)
    except BaseException:
        pass

def test_case_8():
    try:
        bytes_0 = b'\x0f\xb0'
        dict_0 = {bytes_0: bytes_0}
        untokenizer_0 = module_1.Untokenizer()
        str_0 = untokenizer_0.untokenize(dict_0)
    except BaseException:
        pass

def test_case_9():
    try:
        int_0 = 1492
        str_0 = 'B~p$5!e/l@-'
        tuple_0 = (int_0, str_0)
        str_1 = 'oJ&'
        str_2 = ':\n@\t^GL?9x\r\x0c'
        dict_0 = {str_1: tuple_0, str_2: int_0}
        grammar_0 = module_0.Grammar()
        iterator_0 = module_1.generate_tokens(dict_0, grammar_0)
        untokenizer_0 = module_1.Untokenizer()
        untokenizer_0.compat(tuple_0, iterator_0)
    except BaseException:
        pass

def test_case_10():
    try:
        str_0 = "'*E7cb-'\x0c V}/B,%B"
        str_1 = [str_0]
        var_0 = iter(str_1)
        var_1 = var_0.__next__
        iterator_0 = module_1.generate_tokens(var_1)
        var_2 = list(iterator_0)
    except BaseException:
        pass

def test_case_11():
    try:
        str_0 = '(!yUQk@J0>F|^x\tnVm:'
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        iterator_0 = module_1.generate_tokens(var_1)
        var_2 = list(iterator_0)
    except BaseException:
        pass

def test_case_12():
    try:
        str_0 = "r'*E7cb-'\x0c V}/,%B"
        str_1 = [str_0]
        var_0 = iter(str_1)
        var_1 = var_0.__next__
        iterator_0 = module_1.generate_tokens(var_1)
        var_2 = list(iterator_0)
    except BaseException:
        pass

def test_case_13():
    try:
        str_0 = 'Ez#'
        str_1 = [str_0]
        var_0 = iter(str_0)
        var_1 = module_1.maybe()
        var_2 = var_0.__next__
        int_0 = 393
        tuple_0 = (int_0, str_1)
        dict_0 = {}
        untokenizer_0 = module_1.Untokenizer()
        untokenizer_0.compat(tuple_0, dict_0)
        iterator_0 = module_1.generate_tokens(var_2)
        var_3 = list(iterator_0)
        callable_0 = None
        callable_1 = None
        module_1.tokenize(callable_0, callable_1)
    except BaseException:
        pass

def test_case_14():
    try:
        str_0 = '[b[\t\x0cJ\\QJ^XEK6$h".'
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        iterator_0 = module_1.generate_tokens(var_1)
        var_2 = list(iterator_0)
    except BaseException:
        pass