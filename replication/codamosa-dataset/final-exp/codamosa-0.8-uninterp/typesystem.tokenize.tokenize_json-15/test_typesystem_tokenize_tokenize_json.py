# Automatically generated by Pynguin.
import typesystem.tokenize.tokenize_json as module_0

def test_case_0():
    pass

def test_case_1():
    str_0 = '{ "name": "foo" }'
    token_0 = module_0.tokenize_json(str_0)

def test_case_2():
    str_0 = '{"last_name": "Doe", "age": 30, "first_name": "John", "gender": null}'
    token_0 = module_0.tokenize_json(str_0)

def test_case_3():
    str_0 = '1.0'
    token_0 = module_0.tokenize_json(str_0)

def test_case_4():
    str_0 = '{ "test": { "test": { "test": [ { "test": 123 } ] } } }'
    token_0 = module_0.tokenize_json(str_0)

def test_case_5():
    str_0 = '{"a":"b"}'
    token_0 = module_0.tokenize_json(str_0)

def test_case_6():
    str_0 = '{ "name": "foo" }'
    token_0 = module_0.tokenize_json(str_0)
    str_1 = '{"a": 1e-10}'
    token_1 = module_0.tokenize_json(str_1)

def test_case_7():
    str_0 = '{}'
    token_0 = module_0.tokenize_json(str_0)
    token_1 = module_0.tokenize_json(str_0)

def test_case_8():
    str_0 = '{"name" : "Brian"}'
    token_0 = module_0.tokenize_json(str_0)
    var_0 = type(token_0)