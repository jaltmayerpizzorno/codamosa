# Automatically generated by Pynguin.
import blib2to3.pgen2.tokenize as module_0
import blib2to3.pgen2.grammar as module_1

def test_case_0():
    try:
        int_0 = -3172
        module_0.tokenize(int_0)
    except BaseException:
        pass

def test_case_1():
    try:
        untokenizer_0 = module_0.Untokenizer()
        module_0.tokenize(untokenizer_0)
    except BaseException:
        pass

def test_case_2():
    try:
        int_0 = None
        tuple_0 = (int_0, int_0)
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.add_whitespace(tuple_0)
    except BaseException:
        pass

def test_case_3():
    try:
        bytes_0 = b'ua|\x98\xf6\xad\xc9~R_'
        untokenizer_0 = module_0.Untokenizer()
        str_0 = untokenizer_0.untokenize(bytes_0)
    except BaseException:
        pass

def test_case_4():
    try:
        int_0 = 35
        str_0 = '\r'
        tuple_0 = (int_0, str_0)
        set_0 = {str_0}
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.compat(tuple_0, set_0)
    except BaseException:
        pass

def test_case_5():
    try:
        untokenizer_0 = module_0.Untokenizer()
        int_0 = True
        str_0 = "V<|6\nNbM'(zdLZw{t["
        tuple_0 = (int_0, str_0)
        untokenizer_0.compat(tuple_0, str_0)
    except BaseException:
        pass

def test_case_6():
    try:
        callable_0 = None
        tuple_0 = module_0.detect_encoding(callable_0)
    except BaseException:
        pass

def test_case_7():
    try:
        callable_0 = None
        grammar_0 = module_1.Grammar()
        iterator_0 = module_0.generate_tokens(callable_0, grammar_0)
        list_0 = [iterator_0, iterator_0, grammar_0, grammar_0]
        str_0 = module_0.untokenize(list_0)
    except BaseException:
        pass

def test_case_8():
    try:
        int_0 = -3018
        int_1 = 251
        tuple_0 = (int_1, int_0)
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.add_whitespace(tuple_0)
    except BaseException:
        pass

def test_case_9():
    try:
        str_0 = '!;nF+g&g@qaB5=\tDS&\\X'
        untokenizer_0 = module_0.Untokenizer()
        str_1 = untokenizer_0.untokenize(str_0)
    except BaseException:
        pass

def test_case_10():
    try:
        str_0 = 'S|7gN F\\2K8pZ'
        untokenizer_0 = module_0.Untokenizer()
        var_0 = iter(str_0)
        dict_0 = {}
        str_1 = untokenizer_0.untokenize(dict_0)
        var_1 = var_0.__next__
        module_0.tokenize(var_1)
        grammar_0 = module_1.Grammar()
        var_2 = grammar_0.copy()
        set_0 = {var_1}
        module_0.tokenize(set_0, grammar_0)
    except BaseException:
        pass

def test_case_11():
    try:
        str_0 = 'blib2to3'
        untokenizer_0 = module_0.Untokenizer()
        var_0 = module_0.maybe()
        int_0 = -1660
        tuple_0 = (int_0, int_0)
        untokenizer_1 = module_0.Untokenizer()
        untokenizer_1.add_whitespace(tuple_0)
        tuple_1 = (int_0, str_0)
        callable_0 = None
        grammar_0 = module_1.Grammar()
        iterator_0 = module_0.generate_tokens(callable_0, grammar_0)
        untokenizer_1.compat(tuple_1, iterator_0)
    except BaseException:
        pass

def test_case_12():
    try:
        str_0 = '"abc" # comment\n"def"\n123'
        var_0 = iter(str_0)
        var_1 = module_0.maybe()
        var_2 = var_0.__next__
        module_0.tokenize(var_2)
        str_1 = 'X"'
        bytes_0 = b'\xe2\x85Daz#\xdf\xf6'
        list_0 = [bytes_0, bytes_0]
        tuple_0 = (str_1, list_0)
        str_2 = module_0.untokenize(tuple_0)
    except BaseException:
        pass

def test_case_13():
    try:
        str_0 = "]K(_zF't|*<]/jo{?>"
        var_0 = iter(str_0)
        tuple_0 = None
        iterator_0 = module_0.generate_tokens(tuple_0)
        var_1 = var_0.__next__
        module_0.tokenize(var_1)
        module_0.tokenize(iterator_0)
    except BaseException:
        pass

def test_case_14():
    try:
        str_0 = 'S|7 F\\2K8pZ'
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        module_0.tokenize(var_1)
        grammar_0 = module_1.Grammar()
        var_2 = grammar_0.copy()
        set_0 = {var_1}
        module_0.tokenize(set_0, grammar_0)
    except BaseException:
        pass

def test_case_15():
    try:
        str_0 = "|K(_zF't|*<]\x0bjo{?>"
        var_0 = iter(str_0)
        tuple_0 = None
        iterator_0 = module_0.generate_tokens(tuple_0)
        var_1 = var_0.__next__
        module_0.tokenize(var_1)
    except BaseException:
        pass

def test_case_16():
    try:
        str_0 = 'n;yGo\raf68lX8UL+q'
        var_0 = iter(str_0)
        var_1 = module_0.maybe()
        var_2 = var_0.__next__
        module_0.tokenize(var_2)
        str_1 = 'X"'
        grammar_0 = module_1.Grammar()
        var_3 = grammar_0.copy()
        set_0 = {var_0, str_1, var_2}
        module_0.tokenize(set_0, grammar_0)
    except BaseException:
        pass