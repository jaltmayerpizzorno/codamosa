# Automatically generated by Pynguin.
import blib2to3.pgen2.tokenize as module_0
import blib2to3.pgen2.grammar as module_1

def test_case_0():
    try:
        float_0 = -1422.02157
        bytes_0 = b'\x17'
        list_0 = [float_0, bytes_0, float_0]
        str_0 = 'L'
        list_1 = None
        tuple_0 = (str_0, list_1)
        str_1 = 'k Yt\nCm@pIt1sP}Ak4&'
        var_0 = module_0.printtoken(float_0, bytes_0, list_0, tuple_0, str_1)
    except BaseException:
        pass

def test_case_1():
    try:
        callable_0 = None
        module_0.tokenize(callable_0)
    except BaseException:
        pass

def test_case_2():
    try:
        str_0 = 'aOkrAE'
        untokenizer_0 = module_0.Untokenizer()
        str_1 = untokenizer_0.untokenize(str_0)
    except BaseException:
        pass

def test_case_3():
    try:
        callable_0 = None
        tuple_0 = module_0.detect_encoding(callable_0)
    except BaseException:
        pass

def test_case_4():
    try:
        str_0 = 'a, b'
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        iterator_0 = module_0.generate_tokens(var_1)
        untokenizer_0 = module_0.Untokenizer()
        str_1 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass

def test_case_5():
    try:
        str_0 = 'iil"TZyW2}Lw'
        var_0 = iter(str_0)
        var_1 = module_0.group()
        dict_0 = {}
        str_1 = module_0.untokenize(dict_0)
        var_2 = var_0.__next__
        var_3 = module_0.tokenize_loop(var_2, str_0)
    except BaseException:
        pass

def test_case_6():
    try:
        int_0 = -1124
        int_1 = -2469
        tuple_0 = (int_0, int_1)
        untokenizer_0 = module_0.Untokenizer()
        untokenizer_0.add_whitespace(tuple_0)
        var_0 = module_0.group()
        callable_0 = None
        iterator_0 = module_0.generate_tokens(callable_0)
        iterator_1 = module_0.generate_tokens(callable_0)
        grammar_0 = module_1.Grammar()
        iterator_2 = module_0.generate_tokens(callable_0, grammar_0)
        str_0 = untokenizer_0.untokenize(iterator_2)
    except BaseException:
        pass

def test_case_7():
    try:
        var_0 = None
        var_1 = lambda *args: var_0
        str_0 = '>=3'
        int_0 = -3295
        untokenizer_0 = module_0.Untokenizer()
        var_2 = iter(str_0)
        str_1 = "I\r'WGd?"
        bytes_0 = b'\xdc\x83'
        list_0 = [bytes_0]
        tuple_0 = (str_1, list_0)
        str_2 = 'q};=f9yqEs\\$&#Kr\t'
        dict_0 = {bytes_0: bytes_0, int_0: list_0}
        list_1 = [list_0]
        var_3 = module_0.printtoken(tuple_0, str_2, bytes_0, dict_0, list_1)
    except BaseException:
        pass

def test_case_8():
    try:
        str_0 = '>=3'
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        var_2 = module_0.tokenize_loop(var_1, str_0)
    except BaseException:
        pass

def test_case_9():
    try:
        str_0 = '0=0'
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        var_2 = module_0.tokenize_loop(var_1, var_1)
    except BaseException:
        pass

def test_case_10():
    try:
        str_0 = 'T3'
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        var_2 = module_0.tokenize_loop(var_1, str_0)
    except BaseException:
        pass

def test_case_11():
    try:
        str_0 = ''
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        var_2 = module_0.tokenize_loop(var_1, str_0)
    except BaseException:
        pass

def test_case_12():
    try:
        str_0 = '\tdv|4sG1|RE~JRGzY{n'
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        var_2 = module_0.tokenize_loop(var_1, str_0)
    except BaseException:
        pass

def test_case_13():
    try:
        str_0 = '.qg[S$f%\n'
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        var_2 = module_0.tokenize_loop(var_1, str_0)
    except BaseException:
        pass

def test_case_14():
    try:
        callable_0 = None
        grammar_0 = module_1.Grammar()
        list_0 = []
        var_0 = module_0.group(*list_0)
        str_0 = '3iF='
        var_1 = iter(var_0)
        var_2 = var_1.__next__
        grammar_0.dump(str_0)
        var_3 = module_0.tokenize_loop(var_2, callable_0)
    except BaseException:
        pass

def test_case_15():
    try:
        str_0 = 'i}W@%]SC?'
        str_1 = [str_0]
        var_0 = iter(str_1)
        var_1 = var_0.__next__
        iterator_0 = module_0.generate_tokens(var_1)
        untokenizer_0 = module_0.Untokenizer()
        str_2 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass

def test_case_16():
    try:
        str_0 = 'a, b'
        str_1 = [str_0]
        var_0 = iter(str_1)
        var_1 = var_0.__next__
        iterator_0 = module_0.generate_tokens(var_1)
        untokenizer_0 = module_0.Untokenizer()
        str_2 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass

def test_case_17():
    try:
        str_0 = ' _'
        var_0 = iter(str_0)
        var_1 = var_0.__next__
        var_2 = module_0.tokenize_loop(var_1, str_0)
    except BaseException:
        pass

def test_case_18():
    try:
        str_0 = 'i}W.Z%]SC?'
        str_1 = [str_0, str_0, str_0]
        var_0 = iter(str_1)
        var_1 = var_0.__next__
        iterator_0 = module_0.generate_tokens(var_1)
        untokenizer_0 = module_0.Untokenizer()
        str_2 = untokenizer_0.untokenize(iterator_0)
    except BaseException:
        pass